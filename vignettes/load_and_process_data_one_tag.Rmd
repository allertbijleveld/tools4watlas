---
title: "Loading and processing data from one tag"
author: "Allert Bijleveld"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Loading and processing data from one tag}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

# Basic workflow 

The basic workflow using *tools4watlas* for high-throughput WATLAS tracking-data is getting data, basic filtering, processing, cleaning, adding environmental data and selecting data.

After installing the package, load the *tools4watlas* library. 

```{r load, eval=FALSE}
library(tools4watlas)
```

## Getting data

You can get data from locally from a csv or SQLite file, or remote SQL database server.  

#### Local csv file

The function atl_get_data_csv() is a convenient wrapper to load a csv file as a data frame. By default it loads the csv file provided as example data with tools4watlas.

```{r, eval=FALSE}
data <- atl_get_data_csv()
```

#### Local SQLite file

First, the path and file name of the local SQLite database need to be provided. Then, with the established connection, the database can be queried for a particular tags and periods.


```{r, eval=FALSE}
SQLiteDB = paste0("path", "SQLite_db_name", ".sqlite")
MyDBconnection <- RSQLite::dbConnect(RSQLite::SQLite(),SQLiteDB)

data <- atl_get_data(
  tag = 31001002707,
	tracking_time_start = "2022-09-02 01:25:00",
	tracking_time_end = "2022-09-03 13:47:00",
	timezone = "CET",
	SQLiteDB=SQLiteDB,
	use_connection = MyDBconnection
)
```

#### Remote SQL-database

It is also possible to connect directly to a remote host.

```{r, eval=FALSE}
data <- atl_get_data(
	tag = 31001002707,
	tracking_time_start = "2022-09-02 01:25:00",
	tracking_time_end = "2022-09-03 13:47:00",
	timezone = "CET",
	host = "host", 
	database = "db",
	username = "username",
	password = "password")			  
```

## Data explanation

Loading the WATLAS data will provide a data frame with different columns representing:  

*PosID*	=	Unique number for localizations \
*TAG* 	=	11 digit WATLAS tag ID \
*tag*	=	4 digit tag number (character), i.e. last 4 digits of the column 'TAG' \
*TIME*	=	UNIX time (seconds) \
*time*	= 	Timestamp in POSIXct (UTC)
*X*		=	X-ccordinates in meters (utm 31 N) \
*Y*		=	Y-ccordinates in meters (utm 31 N) \
*NBS*	=	Number of Base Stations used in calculating coordinates \
*VARX*	=	Variance in estimating X-coordinates \
*VARY*	=	Variance in estimating Y-coordinates \
*COVXY*	=	Co-variance between X- and Y-coordinates \

## Spatiotemporal filtering 

After getting the data, a potential first step is applying basic filtering to select certain areas of interest, or remove areas with erroneous localizations. 

Here, is an example of removing hypothesized erroneous localizations from a rectangular area specified with the range in x and y coordinates, but a spatial polygon could also be used (see *?atl_filter_bounds*). 

```{r, eval=FALSE}
data <- atl_filter_bounds(
	data = data,
	x_range = c(639470, 639471),
	y_range = c(5887143, 5887144),
	sf_polygon = NULL,
	remove_inside = TRUE
)
```

For filtering data, the general *atl_filter_covariates* function can also be used. For example, filtering on a range of coordinates and a time period:  

```{r, eval=FALSE}
data <- atl_filter_covariates(
	data = data,
	filters = c(
	"between(time, '2022-09-02 01:25:00', '2022-09-03 13:47:00')",
	"between(X, 649686, 651938)") 
)				
```

## Basic processing 

With the data of interest, some basic variables can be calculated, for instance, calculating speeds and turning angles from consecutive localizations. Speed can then later be used for filtering potentially erroneous localizations.

```{r, eval=FALSE}					
#> calculate speed between consecutive localizations  		
data$speed_in <- atl_get_speed(data = data, time = "TIME", type = "in") 
data$speed_out <- atl_get_speed(data = data, time = "TIME", type = "out") 

#> calculate angle between consecutive localizations  		
data$angle <- atl_turning_angle(data = data, time = "TIME") 
```

## Filtering 

The next step is to remove localization errors, for instance, by applying basic filtering on the variances in estimating x- and y-coordinates and speed.  

```{r, eval=FALSE}
VARmax	<- 5000	# variance in meters squared
speed_max <- 35 # meters per second

data <- atl_filter_covariates(
	data = data,
	filters = c(
		"VARX < VARmax", 
		"VARY < VARmax",
		"speed_in < speed_max",
		"speed_out < speed_max"
	  )
)
```

## Smoothing  

To further reduce error in the localization data, a basic smoother such as a median filter can be applied.  

```{r, eval=FALSE}
med_filter <- 5	# window for smoothing localizations
data <- atl_median_smooth(data = data, time = "TIME", moving_window = med_filter)
```

After smoothing the data, the speeds and angles need to be recalculated.

```{r, eval=FALSE}					
data$speed_in <- atl_get_speed(data=data, time="TIME", type = "in") 
data$speed_out <- atl_get_speed(data=data, time="TIME", type = "out") 

#> Note that the distance between smooted localization can be zero, and 
#> therefore, the angle cannot be calculated and a warning and NaNs are returned 	
data$angle <- atl_turning_angle(data = data, time = "TIME") 
```

## Adding tidal data 

After following the above basic steps, the data will be ready for adding environmental data, such as waterlevels.

```{r, eval=FALSE}
#> load the tidal data using the library data.table
tides_filename <- system.file("extdata", "example_tide_data_UTC.csv", 
                              package="tools4watlas")
tide_data_highres_filename <- system.file("extdata", 
                                          "example_tide_data_highres_UTC.csv", 
                                          package = "tools4watlas")
tides <- data.table::fread(tides_filename)
tide_data_highres <- data.table::fread(tide_data_highres_filename)

#> add the tidal data to the tracking data. Note that we use an offset 
#> of 30 min becasue of the delay in water flow between the tidal gauge 
#> and the location of the example tracking data (the islet of Griend)

data <- atl_add_tidaldata(data = data, 
                          tide_data = tides, 
                          tide_data_highres = tide_data_highres, 
                          waterdata_resolution = "10 minute", 
                          Offset = 30)
```


## Data selection  

For specific analyses, the cleaned data can be selected. To select localizations when mudlfats are available for foraging, we can for example select a low tide period from -2.5 hours to +2.5 hours around low tide [(Bijleveld et al. 2016)](https://royalsocietypublishing.org/doi/10.1098/rspb.2015.1557): 

```{r, eval=FALSE}
#> select the low tide periode for a particular tide as specified by tideID	
data <- atl_filter_covariates(
	data = data,
	filters = c(
	"tideID == 2022472",
	"between(time2lowtide, -2.5 * 60, 2.5 * 60)")
)