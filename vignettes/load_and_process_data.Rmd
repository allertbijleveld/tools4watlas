---
title: "Loading and processing data"
author: "Allert Bijleveld & Johannes Krietsch"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Loading and processing data}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

# Basic workflow 

The basic workflow using *tools4watlas* for high-throughput WATLAS tracking-data is getting data, basic filtering, processing, cleaning, adding environmental data and selecting data.

After installing the package, load the *tools4watlas* library. 

```{r}
library(tools4watlas)
```

## Getting data

You can get data from locally from a csv or SQLite file, or remote SQL database server.  

### Local csv file

The function atl_get_data_csv() is a convenient wrapper to load a csv file as a data frame. By default it loads the csv file provided as example data with tools4watlas.

```{r}
data <- atl_get_data_csv()
```

### SQL database

First select the tags and time period.

```{r}
# Tags for which data should be extracted form the database
tags <- c(2707, 2708)
tags <- atl_full_tag_id(tags) # Change to long tag format (as in database)

# Time period for which data should be extracted form the database
from <- "2023-01-01 12:00:00"
to   <- "2024-01-01 12:00:00"
```

Alternatively, this selection can also be based on the tags_watlas_all.xlsx file (including metadata of all tags) and the time period can be specified as last days (only useful when accessing remote SQL database)

```{r, eval=FALSE}
# Load Excel file 
  # provide the correct path and file name, and sheet of the excel file. 
library(readxl)

alltags <- readxl::read_excel("C:\\path\\tags_watlas_all.xlsx", 
                              sheet = "tags_watlas_all") |> 
  data.table()

# Subset tag ID's using data.table 
# (For example red knots, dunlin and sanderling tagged in 2023)
tags <- alltags[season == 2023 & 
                 species %in% c("red knot", "dunlin", "sanderling")]$tagID

# Change to long tag format
tags <- atl_full_tag_id(tags) # Change to long tag format (as in database)

# Select N last days to get data from
days <- 3
from <- Sys.time() - 86400 * days |> as.character()
to   <- Sys.time() + 3600 |> as.character()
```


#### Local SQLite file

First, the path and file name of the local SQLite database need to be provided. Then, with the established connection, the database can be queried for the selected tags and period. Here we will load the tagging data in a list where each entry is a dataframe with data of one tag.


```{r, eval=FALSE}
SQLiteDB = paste0("path", "SQLite_db_name", ".sqlite")
MyDBconnection <- RSQLite::dbConnect(RSQLite::SQLite(),SQLiteDB)

ldf_raw <- lapply(
  tags, 
  atl_get_data, 
  tracking_time_start = from, 
  tracking_time_end = to, 
  timezone = "CET", 
  SQLiteDB = mydb, 
  use_connection = MyDBconnection
) 
```

#### Remote SQL-database

It is also possible to connect directly to a remote host.

```{r, eval=FALSE}
ldf_raw  <- lapply(
  tags, 
  atl_get_data,
  tracking_time_start = from,
  tracking_time_end = to,
  timezone = "CET",
  host= "host", 
  database = "db",
  username = "username",
  password = "password"
)  			  
```

## Data explanation

The resulting loaded WATLAS data will look like this: 

```{r, layout="l-body-outset"}
head(data) |> knitr::kable(digits = 2)
```


Loading the WATLAS data will provide a data frame with different columns representing:  

| Column  | Description |
| ----    | --------------------- |
|*PosID*  |	Unique number for localizations  |
|*TAG*    |	11 digit WATLAS tag ID  |
|*tag*	  |	4 digit tag number (character), i.e. last 4 digits of the column 'TAG'  |
|*TIME*	  |	UNIX time (seconds)  |
|*time*	  | Timestamp in POSIXct (UTC)  |
|*X*      |	X-ccordinates in meters (utm 31 N)  |
|*Y*		  |	Y-ccordinates in meters (utm 31 N)  |
|*NBS*	  |	Number of Base Stations used in calculating coordinates  |
|*VARX*	  |	Variance in estimating X-coordinates  |
|*VARY*	  |	Variance in estimating Y-coordinates  |
|*COVXY*	|	Co-variance between X- and Y-coordinates  |

## Spatiotemporal filtering 

After getting the data, a potential first step is applying basic filtering to select certain areas of interest, or remove areas with erroneous localizations. 

Here, is an example of removing hypothesized erroneous localizations from a rectangular area specified with the range in x and y coordinates, but a spatial polygon could also be used (see *?atl_filter_bounds*). 

```{r, eval=FALSE}
# @ALLERT make run on lists? This does not seem to work
ldf_filtered <- lapply(
  ldf_raw,
  atl_filter_bounds,
  x_range = c(639470, 639471),
  y_range = c(5887143, 5887144),
  sf_polygon = NULL,
  remove_inside = TRUE
)
```

For filtering data, the general *atl_filter_covariates* function can also be used. For example, filtering on a range of coordinates and a time period:  

```{r, eval=FALSE}
ldf_filtered <- lapply(
  ldf_raw, 
	atl_filter_covariates,
  filters = c(
    "between(time, '2022-09-02 01:25:00', '2022-09-03 13:47:00')",
	  "between(X, 649686, 651938)"
    ) 
  )
```

## Basic processing 

With the data of interest, some basic variables can be calculated, for instance, calculating speeds and turning angles from consecutive localizations. Speed can then later be used for filtering potentially erroneous localizations.

```{r, eval=FALSE}					
# @ALLERT make run on lists? This does not seem to work

# Calculate speed between consecutive localizations  		
data$speed_in <- atl_get_speed(data = data, time = "TIME", type = "in") 
data$speed_out <- atl_get_speed(data = data, time = "TIME", type = "out") 

#  Calculate angle between consecutive localizations  		
data$angle <- atl_turning_angle(data = data, time = "TIME") 
```

## Filtering 

The next step is to remove localization errors, for instance, by applying basic filtering on the variances in estimating x- and y-coordinates and speed.  

```{r, eval=FALSE}
VARmax	<- 5000	# variance in meters squared
speed_max <- 35 # meters per second

# ldf_clean <- lapply(
#   ldf_raw, 
# 	atl_filter_covariates,
# 	filters = c(
# 	  "VARX < VARmax", 
# 		"VARY < VARmax",
# 		"speed_in < speed_max",
# 		"speed_out < speed_max"
# 		)
#   )

VARmax	<- 5000	# Variance limit in meters squared

ldf_clean <- lapply(
  ldf_raw, 
	atl_filter_covariates,
	filters = c(
	  "VARX < VARmax", 
		"VARY < VARmax"
		)
  )
```

## Smoothing  

To further reduce error in the localization data, a basic smoother such as a median filter can be applied.  

```{r, eval=FALSE}
med_filter <- 5	# number of localizations within window for smoothing

ldf_smoothed <- lapply(
  ldf_clean,
  atl_median_smooth,
  time = "TIME", 
  moving_window = med_filter
)
```

After smoothing the data, the speeds and angles need to be recalculated.

```{r, eval=FALSE}					
data$speed_in <- atl_get_speed(data = data, time = "TIME", type = "in") 
data$speed_out <- atl_get_speed(data = data, time = "TIME", type = "out") 

#> Note that the distance between smooted localization can be zero, and 
#> therefore, the angle cannot be calculated and a warning and NaNs are returned 	
data$angle <- atl_turning_angle(data = data, time = "TIME") 
```

## Adding tidal data 

After following the above basic steps, the data will be ready for adding environmental data, such as waterlevels.

```{r, eval=FALSE}
#> load the tidal data using the library data.table
tides_filename <- system.file("extdata", "example_tide_data_UTC.csv", 
                              package="tools4watlas")
tide_data_highres_filename <- system.file("extdata", 
                                          "example_tide_data_highres_UTC.csv", 
                                          package = "tools4watlas")
tides <- data.table::fread(tides_filename)
tide_data_highres <- data.table::fread(tide_data_highres_filename)

#> add the tidal data to the tracking data. Note that we use an offset 
#> of 30 min becasue of the delay in water flow between the tidal gauge 
#> and the location of the example tracking data (the islet of Griend)

data <- atl_add_tidaldata(data = data, 
                          tide_data = tides, 
                          tide_data_highres = tide_data_highres, 
                          waterdata_resolution = "10 minute", 
                          Offset = 30)
```


## Data selection  

For specific analyses, the cleaned data can be selected. To select localizations when mudlfats are available for foraging, we can for example select a low tide period from -2.5 hours to +2.5 hours around low tide [(Bijleveld et al. 2016)](https://royalsocietypublishing.org/doi/10.1098/rspb.2015.1557): 

```{r, eval=FALSE}
#> select the low tide periode for a particular tide as specified by tideID	
data <- atl_filter_covariates(
	data = data,
	filters = c(
	"tideID == 2022472",
	"between(time2lowtide, -2.5 * 60, 2.5 * 60)")
)