---
title: "Processing data"
author: "Allert Bijleveld & Johannes Krietsch"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Processing data}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

# Basic workflow 

The basic workflow using *tools4watlas* for high-throughput WATLAS tracking-data is getting data, basic filtering, processing, cleaning, adding environmental data and selecting data.

After installing the package, load the *tools4watlas* library. 

```{r}
library(tools4watlas)
library(data.table)
```

## Getting data

You can get data from locally from a csv or SQLite file, or remote SQL database server.  

### Local csv file

The function atl_get_data_csv() is a convenient wrapper to load a csv file as a data frame. By default it loads the csv file provided as example data with tools4watlas.

```{r}
data <- atl_get_data_csv() |> 
  data.table()

# Split data by ID
data_split <- split(data, by = "TAG")
```


## Spatiotemporal filtering 

After getting the data, a potential first step is applying basic filtering to select certain areas of interest, or remove areas with erroneous localizations. 

Here, is an example of removing hypothesized erroneous localizations from a rectangular area specified with the range in x and y coordinates, but a spatial polygon could also be used (see *?atl_filter_bounds*). 

```{r, eval=FALSE}
# @ALLERT make run on lists? This does not seem to work
ldf_filtered <- lapply(
  ldf_raw,
  atl_filter_bounds,
  x_range = c(639470, 639471),
  y_range = c(5887143, 5887144),
  sf_polygon = NULL,
  remove_inside = TRUE
)
```

For filtering data, the general *atl_filter_covariates* function can also be used. For example, filtering on a range of coordinates and a time period:  

```{r, eval=FALSE}
ldf_filtered <- lapply(
  ldf_raw, 
	atl_filter_covariates,
  filters = c(
    "between(time, '2022-09-02 01:25:00', '2022-09-03 13:47:00')",
	  "between(X, 649686, 651938)"
    ) 
  )
```

## Basic processing 

Transform the lists in data.tables.

```{r, eval=FALSE}	
lapply(data_split, setDT) |> 
  invisible()
```


With the data of interest, some basic variables can be calculated, for instance, calculating speeds and turning angles from consecutive localizations. Speed can then later be used for filtering potentially erroneous localizations.

```{r, eval=FALSE}					
# calculate SD
lapply(data_split, function(dt) {
  dt[, SD := sqrt(VARX + VARY + (2 * COVXY))]
}) |> 
  invisible()

# calculate speed
lapply(data_split, function(dt) {
  dt[, `:=`(
    speed_in = atl_get_speed(dt, time = "TIME", type = "in"),
    speed_out = atl_get_speed(dt, time = "TIME", type = "out")
  )]
}) |> 
  invisible()
```

## Filtering 

The next step is to remove localization errors, for instance, by applying basic filtering on the variances in estimating x- and y-coordinates and speed.  

```{r, eval=FALSE}
VARmax	<- 5000	# variance in meters squared
speed_max <- 35 # meters per second

data_split <- lapply(data_split, function(dt) {
  dt <- atl_filter_covariates(
    data = dt,
    filters = c(
	  "VARX < VARmax",
		"VARY < VARmax",
		"speed_in < speed_max",
		"speed_out < speed_max"
    )
  )
})
```

## Smoothing  

To further reduce error in the localization data, a basic smoother such as a median filter can be applied.  

```{r, eval=FALSE}
med_filter <- 5	# number of localizations within window for smoothing

# since the function modifies in place, we shall make a copy
data_smooth <- copy(data_split)

# make lists data.tables
lapply(data_smooth, setDT) |>
  invisible()

lapply(
  X = data_smooth,
  FUN = atl_median_smooth,
  time = "TIME", moving_window = med_filter
) |> 
  invisible()
```

After smoothing the data, the speeds and angles need to be recalculated.

```{r, eval=FALSE}					
lapply(data_smooth, function(dt) {
  dt[, `:=`(
    speed_in = atl_get_speed(dt, time = "TIME", type = "in"),
    speed_out = atl_get_speed(dt, time = "TIME", type = "out")
  )]
  dt[, angle := atl_turning_angle(dt, time = "TIME")]
}) |> 
  invisible() 
```

## Adding tidal data 

After following the above basic steps, the data will be ready for adding environmental data, such as waterlevels.

```{r, eval=FALSE}

data = rbindlist(data_smooth, fill = TRUE)

# add water level data
tides <- system.file(
  "extdata", "example_tide_data_UTC.csv", package = "tools4watlas")
tide_data_highres <- system.file(
  "extdata", "example_tide_data_highres_UTC.csv", package = "tools4watlas"
  )
data = as.data.frame(data)
data = atl_add_tidaldata(
  data = d,
  tide_data = tides, 
  tide_data_highres = tide_data_highres, 
  waterdata_resolution = "10 minute",
  Offset = 30)
```


## Data selection  

For specific analyses, the cleaned data can be selected. To select localizations when mudlfats are available for foraging, we can for example select a low tide period from -2.5 hours to +2.5 hours around low tide [(Bijleveld et al. 2016)](https://royalsocietypublishing.org/doi/10.1098/rspb.2015.1557): 

```{r, eval=FALSE}
# Split data by ID
data_split <- split(data, by = "TAG")

# Select the low tide period for a particular tide as specified by tideID	
data_split <- lapply(data_split, function(dt) {
  dt <- atl_filter_covariates(
    data = dt,
    filters = c(
  	"tideID == 2022472",
  	"between(time2lowtide, -2.5 * 60, 2.5 * 60)"
    )
  )
})
```
